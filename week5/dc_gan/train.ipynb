{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59f687c",
   "metadata": {},
   "source": [
    "Training of DCGAN network on MNIST dataset with Discriminator\n",
    "and Generator imported from models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb1e47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model import Discriminator, Generator, initialise_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7b0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=True, transform=transforms, download=True\n",
    ")\n",
    "\n",
    "# comment mnist above and uncomment below if train on CelebA\n",
    "# dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialise_weights(gen)\n",
    "initialise_weights(disc)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d347ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Batch 1/469               Loss D: 0.7050, loss G: 0.7408\n",
      "Epoch [1/5] Batch 2/469               Loss D: 0.6494, loss G: 0.8091\n",
      "Epoch [1/5] Batch 3/469               Loss D: 0.6066, loss G: 0.8678\n",
      "Epoch [1/5] Batch 4/469               Loss D: 0.5669, loss G: 0.9202\n",
      "Epoch [1/5] Batch 5/469               Loss D: 0.5299, loss G: 0.9704\n",
      "Epoch [1/5] Batch 6/469               Loss D: 0.4983, loss G: 1.0173\n",
      "Epoch [1/5] Batch 7/469               Loss D: 0.4673, loss G: 1.0656\n",
      "Epoch [1/5] Batch 8/469               Loss D: 0.4346, loss G: 1.1145\n",
      "Epoch [1/5] Batch 9/469               Loss D: 0.4069, loss G: 1.1621\n",
      "Epoch [1/5] Batch 10/469               Loss D: 0.3851, loss G: 1.2111\n",
      "Epoch [1/5] Batch 11/469               Loss D: 0.3592, loss G: 1.2593\n",
      "Epoch [1/5] Batch 12/469               Loss D: 0.3386, loss G: 1.3072\n",
      "Epoch [1/5] Batch 13/469               Loss D: 0.3157, loss G: 1.3551\n",
      "Epoch [1/5] Batch 14/469               Loss D: 0.2976, loss G: 1.4034\n",
      "Epoch [1/5] Batch 15/469               Loss D: 0.2773, loss G: 1.4526\n",
      "Epoch [1/5] Batch 16/469               Loss D: 0.2629, loss G: 1.5003\n",
      "Epoch [1/5] Batch 17/469               Loss D: 0.2470, loss G: 1.5480\n",
      "Epoch [1/5] Batch 18/469               Loss D: 0.2324, loss G: 1.5957\n",
      "Epoch [1/5] Batch 19/469               Loss D: 0.2193, loss G: 1.6434\n",
      "Epoch [1/5] Batch 20/469               Loss D: 0.2075, loss G: 1.6907\n",
      "Epoch [1/5] Batch 21/469               Loss D: 0.1964, loss G: 1.7366\n",
      "Epoch [1/5] Batch 22/469               Loss D: 0.1862, loss G: 1.7798\n",
      "Epoch [1/5] Batch 23/469               Loss D: 0.1755, loss G: 1.8224\n",
      "Epoch [1/5] Batch 24/469               Loss D: 0.1655, loss G: 1.8638\n",
      "Epoch [1/5] Batch 25/469               Loss D: 0.1585, loss G: 1.9041\n",
      "Epoch [1/5] Batch 26/469               Loss D: 0.1505, loss G: 1.9422\n",
      "Epoch [1/5] Batch 27/469               Loss D: 0.1437, loss G: 1.9820\n",
      "Epoch [1/5] Batch 28/469               Loss D: 0.1366, loss G: 2.0207\n",
      "Epoch [1/5] Batch 29/469               Loss D: 0.1314, loss G: 2.0616\n",
      "Epoch [1/5] Batch 30/469               Loss D: 0.1232, loss G: 2.1047\n",
      "Epoch [1/5] Batch 31/469               Loss D: 0.1188, loss G: 2.1464\n",
      "Epoch [1/5] Batch 32/469               Loss D: 0.1134, loss G: 2.1846\n",
      "Epoch [1/5] Batch 33/469               Loss D: 0.1085, loss G: 2.2222\n",
      "Epoch [1/5] Batch 34/469               Loss D: 0.1035, loss G: 2.2593\n",
      "Epoch [1/5] Batch 35/469               Loss D: 0.0992, loss G: 2.2984\n",
      "Epoch [1/5] Batch 36/469               Loss D: 0.0959, loss G: 2.3361\n",
      "Epoch [1/5] Batch 37/469               Loss D: 0.0912, loss G: 2.3738\n",
      "Epoch [1/5] Batch 38/469               Loss D: 0.0876, loss G: 2.4101\n",
      "Epoch [1/5] Batch 39/469               Loss D: 0.0839, loss G: 2.4459\n",
      "Epoch [1/5] Batch 40/469               Loss D: 0.0803, loss G: 2.4812\n",
      "Epoch [1/5] Batch 41/469               Loss D: 0.0774, loss G: 2.5157\n",
      "Epoch [1/5] Batch 42/469               Loss D: 0.0743, loss G: 2.5512\n",
      "Epoch [1/5] Batch 43/469               Loss D: 0.0717, loss G: 2.5856\n",
      "Epoch [1/5] Batch 44/469               Loss D: 0.0690, loss G: 2.6185\n",
      "Epoch [1/5] Batch 45/469               Loss D: 0.0666, loss G: 2.6520\n",
      "Epoch [1/5] Batch 46/469               Loss D: 0.0639, loss G: 2.6847\n",
      "Epoch [1/5] Batch 47/469               Loss D: 0.0615, loss G: 2.7174\n",
      "Epoch [1/5] Batch 48/469               Loss D: 0.0598, loss G: 2.7489\n",
      "Epoch [1/5] Batch 49/469               Loss D: 0.0581, loss G: 2.7806\n",
      "Epoch [1/5] Batch 50/469               Loss D: 0.0558, loss G: 2.8117\n",
      "Epoch [1/5] Batch 51/469               Loss D: 0.0538, loss G: 2.8419\n",
      "Epoch [1/5] Batch 52/469               Loss D: 0.0520, loss G: 2.8720\n",
      "Epoch [1/5] Batch 53/469               Loss D: 0.0500, loss G: 2.9015\n",
      "Epoch [1/5] Batch 54/469               Loss D: 0.0487, loss G: 2.9311\n",
      "Epoch [1/5] Batch 55/469               Loss D: 0.0473, loss G: 2.9605\n",
      "Epoch [1/5] Batch 56/469               Loss D: 0.0459, loss G: 2.9887\n",
      "Epoch [1/5] Batch 57/469               Loss D: 0.0440, loss G: 3.0178\n",
      "Epoch [1/5] Batch 58/469               Loss D: 0.0434, loss G: 3.0450\n",
      "Epoch [1/5] Batch 59/469               Loss D: 0.0422, loss G: 3.0725\n",
      "Epoch [1/5] Batch 60/469               Loss D: 0.0410, loss G: 3.0964\n",
      "Epoch [1/5] Batch 61/469               Loss D: 0.0408, loss G: 3.1260\n",
      "Epoch [1/5] Batch 62/469               Loss D: 0.0405, loss G: 3.1533\n",
      "Epoch [1/5] Batch 63/469               Loss D: 0.0391, loss G: 3.1746\n",
      "Epoch [1/5] Batch 64/469               Loss D: 0.0378, loss G: 3.2139\n",
      "Epoch [1/5] Batch 65/469               Loss D: 0.0367, loss G: 3.2247\n",
      "Epoch [1/5] Batch 66/469               Loss D: 0.0359, loss G: 3.2642\n",
      "Epoch [1/5] Batch 67/469               Loss D: 0.0350, loss G: 3.2869\n",
      "Epoch [1/5] Batch 68/469               Loss D: 0.0340, loss G: 3.3063\n",
      "Epoch [1/5] Batch 69/469               Loss D: 0.0330, loss G: 3.3272\n",
      "Epoch [1/5] Batch 70/469               Loss D: 0.0324, loss G: 3.3619\n",
      "Epoch [1/5] Batch 71/469               Loss D: 0.0320, loss G: 3.3409\n",
      "Epoch [1/5] Batch 72/469               Loss D: 0.0318, loss G: 3.4005\n",
      "Epoch [1/5] Batch 73/469               Loss D: 0.0298, loss G: 3.4313\n",
      "Epoch [1/5] Batch 74/469               Loss D: 0.0288, loss G: 3.4497\n",
      "Epoch [1/5] Batch 75/469               Loss D: 0.0281, loss G: 3.4780\n",
      "Epoch [1/5] Batch 76/469               Loss D: 0.0273, loss G: 3.5048\n",
      "Epoch [1/5] Batch 77/469               Loss D: 0.0266, loss G: 3.5252\n",
      "Epoch [1/5] Batch 78/469               Loss D: 0.0264, loss G: 3.5431\n",
      "Epoch [1/5] Batch 79/469               Loss D: 0.0264, loss G: 3.5710\n",
      "Epoch [1/5] Batch 80/469               Loss D: 0.0253, loss G: 3.5904\n",
      "Epoch [1/5] Batch 81/469               Loss D: 0.0253, loss G: 3.6194\n",
      "Epoch [1/5] Batch 82/469               Loss D: 0.0244, loss G: 3.6377\n",
      "Epoch [1/5] Batch 83/469               Loss D: 0.0237, loss G: 3.6571\n",
      "Epoch [1/5] Batch 84/469               Loss D: 0.0240, loss G: 3.6824\n",
      "Epoch [1/5] Batch 85/469               Loss D: 0.0241, loss G: 3.6798\n",
      "Epoch [1/5] Batch 86/469               Loss D: 0.0235, loss G: 3.7439\n",
      "Epoch [1/5] Batch 87/469               Loss D: 0.0217, loss G: 3.7583\n",
      "Epoch [1/5] Batch 88/469               Loss D: 0.0212, loss G: 3.7724\n",
      "Epoch [1/5] Batch 89/469               Loss D: 0.0205, loss G: 3.8001\n",
      "Epoch [1/5] Batch 90/469               Loss D: 0.0201, loss G: 3.8214\n",
      "Epoch [1/5] Batch 91/469               Loss D: 0.0194, loss G: 3.8410\n",
      "Epoch [1/5] Batch 92/469               Loss D: 0.0190, loss G: 3.8547\n",
      "Epoch [1/5] Batch 93/469               Loss D: 0.0189, loss G: 3.8829\n",
      "Epoch [1/5] Batch 94/469               Loss D: 0.0183, loss G: 3.8961\n",
      "Epoch [1/5] Batch 95/469               Loss D: 0.0179, loss G: 3.9156\n",
      "Epoch [1/5] Batch 96/469               Loss D: 0.0173, loss G: 3.9415\n",
      "Epoch [1/5] Batch 97/469               Loss D: 0.0173, loss G: 3.9652\n",
      "Epoch [1/5] Batch 98/469               Loss D: 0.0167, loss G: 3.9815\n",
      "Epoch [1/5] Batch 99/469               Loss D: 0.0163, loss G: 4.0029\n",
      "Epoch [1/5] Batch 100/469               Loss D: 0.0160, loss G: 4.0170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m real \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(BATCH_SIZE, NOISE_DIM, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m fake \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m disc_real \u001b[38;5;241m=\u001b[39m disc(real)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/soc GANS/dc_gan/model.py:59\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noise):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Batch {batch_idx+1}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5883ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
