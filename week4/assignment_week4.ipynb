{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948121dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3cda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# load data\n",
    "data = torchvision.datasets.MNIST(root='./MNIST_data', train=True, transform = transforms.ToTensor(), download=True)\n",
    "target = torchvision.datasets.MNIST(root='./MNIST_data', train=False , transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ef580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=target,batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=target, batch_size=batch_size, shuffle=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ca7a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.shape, labels.shape)\n",
    "# 100 samples and 100 labels -> each sample have a single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3de0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # nn.Conv2d(no. of input chanels, num output chanels, kernel size)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1,6,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6,16,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Linear(16*4*4,10)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # reshape x to flatten\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "model = ConvNet()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaecb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c05c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10 step 100/600 loss: 0.58547043800354\n",
      "epoch 1/10 step 200/600 loss: 0.32082921266555786\n",
      "epoch 1/10 step 300/600 loss: 0.14249181747436523\n",
      "epoch 1/10 step 400/600 loss: 0.1799234002828598\n",
      "epoch 1/10 step 500/600 loss: 0.10537133365869522\n",
      "epoch 1/10 step 600/600 loss: 0.1073470413684845\n",
      "epoch 2/10 step 100/600 loss: 0.12023258209228516\n",
      "epoch 2/10 step 200/600 loss: 0.08195315301418304\n",
      "epoch 2/10 step 300/600 loss: 0.06333418935537338\n",
      "epoch 2/10 step 400/600 loss: 0.14010734856128693\n",
      "epoch 2/10 step 500/600 loss: 0.0589279904961586\n",
      "epoch 2/10 step 600/600 loss: 0.11757534742355347\n",
      "epoch 3/10 step 100/600 loss: 0.09426600486040115\n",
      "epoch 3/10 step 200/600 loss: 0.14024010300636292\n",
      "epoch 3/10 step 300/600 loss: 0.07367437332868576\n",
      "epoch 3/10 step 400/600 loss: 0.14022165536880493\n",
      "epoch 3/10 step 500/600 loss: 0.07072710245847702\n",
      "epoch 3/10 step 600/600 loss: 0.03702051565051079\n",
      "epoch 4/10 step 100/600 loss: 0.039491429924964905\n",
      "epoch 4/10 step 200/600 loss: 0.027049539610743523\n",
      "epoch 4/10 step 300/600 loss: 0.13918542861938477\n",
      "epoch 4/10 step 400/600 loss: 0.05700080841779709\n",
      "epoch 4/10 step 500/600 loss: 0.1937418431043625\n",
      "epoch 4/10 step 600/600 loss: 0.059558164328336716\n",
      "epoch 5/10 step 100/600 loss: 0.10284330695867538\n",
      "epoch 5/10 step 200/600 loss: 0.07533978670835495\n",
      "epoch 5/10 step 300/600 loss: 0.12469969689846039\n",
      "epoch 5/10 step 400/600 loss: 0.06956753134727478\n",
      "epoch 5/10 step 500/600 loss: 0.03587066009640694\n",
      "epoch 5/10 step 600/600 loss: 0.10213346779346466\n",
      "epoch 6/10 step 100/600 loss: 0.03177670016884804\n",
      "epoch 6/10 step 200/600 loss: 0.05932778865098953\n",
      "epoch 6/10 step 300/600 loss: 0.06327345222234726\n",
      "epoch 6/10 step 400/600 loss: 0.038833584636449814\n",
      "epoch 6/10 step 500/600 loss: 0.03068654052913189\n",
      "epoch 6/10 step 600/600 loss: 0.06508629024028778\n",
      "epoch 7/10 step 100/600 loss: 0.03721557930111885\n",
      "epoch 7/10 step 200/600 loss: 0.07335551828145981\n",
      "epoch 7/10 step 300/600 loss: 0.10174216330051422\n",
      "epoch 7/10 step 400/600 loss: 0.062255147844552994\n",
      "epoch 7/10 step 500/600 loss: 0.07373104244470596\n",
      "epoch 7/10 step 600/600 loss: 0.04922529309988022\n",
      "epoch 8/10 step 100/600 loss: 0.04279550164937973\n",
      "epoch 8/10 step 200/600 loss: 0.0878525823354721\n",
      "epoch 8/10 step 300/600 loss: 0.02209693193435669\n",
      "epoch 8/10 step 400/600 loss: 0.01837804727256298\n",
      "epoch 8/10 step 500/600 loss: 0.05709948390722275\n",
      "epoch 8/10 step 600/600 loss: 0.044160932302474976\n",
      "epoch 9/10 step 100/600 loss: 0.024347104132175446\n",
      "epoch 9/10 step 200/600 loss: 0.02336823008954525\n",
      "epoch 9/10 step 300/600 loss: 0.017017634585499763\n",
      "epoch 9/10 step 400/600 loss: 0.04605729877948761\n",
      "epoch 9/10 step 500/600 loss: 0.016305427998304367\n",
      "epoch 9/10 step 600/600 loss: 0.04904796928167343\n",
      "epoch 10/10 step 100/600 loss: 0.07634130120277405\n",
      "epoch 10/10 step 200/600 loss: 0.02324729599058628\n",
      "epoch 10/10 step 300/600 loss: 0.060607872903347015\n",
      "epoch 10/10 step 400/600 loss: 0.019285773858428\n",
      "epoch 10/10 step 500/600 loss: 0.02475718781352043\n",
      "epoch 10/10 step 600/600 loss: 0.004557506181299686\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # curr images.shape -> 100,1,28,28\n",
    "        # change it to -> 100,784\n",
    "#         images = images.reshape(-1,784)\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(images)\n",
    "        \n",
    "        # loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimiser.step()\n",
    "\n",
    "        # reload gradients to zero\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        if (i+1)%100 == 0: print(f'epoch {epoch+1}/{num_epochs} step {i+1}/{len(train_loader)} loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76851729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 98.74%\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # value, index\n",
    "        _, predictions = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum()\n",
    "        \n",
    "    acc = 100.0*n_correct/n_samples\n",
    "    print(f'accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57640966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
